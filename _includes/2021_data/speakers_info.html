<!-- Portfolio Grid Section -->
    <section id="organizers" class="bg-white">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Keynote Speakers</h2>
<!--                     <h3 class="section-subheading text-muted">TBD</h3> -->
                </div>
            </div>
            <form id="searchForm" class="col-md-12 form-horizontal" role="form">
                <div class="row">
                    <div class="col-md-10">
                        <p class="large">
                        <strong><a href="http://gr.xjtu.edu.cn/web/ygong">Prof. Yihong Gong</a></strong> is a full Professor at the College of Software Engineering, Xi’an Jiaotong University, Xi’an, China. He received the B.S., M.S., and Ph.D. degrees in electrical and electronic engineering from The University of Tokyo, Tokyo, Japan, in 1987, 1989, and 1992, respectively. He was an Assistant Professor with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, for a period of four years. From 1996 to 1998, he was a Project Scientist with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA. He joined NEC Laboratories America, Princeton, NJ, USA, in 1999, and established the Media Analytics Group for the laboratories, where he became the Site Manager to lead the entire branch at Cupertino, CA, USA. He joined Xi’an Jiaotong University, Xi’an, China, in 2012, and became a Distinguished Professor with the National Thousand Talents Program, the Vice Director of the National Engineering Laboratory for Visual Information Processing, and the Chief Scientist of the China National Key Basic Research Project (973 Project). His current research interests include pattern recognition, machine learning, and multimedia content analysis. He is the Fellow of IEEE.</p>
                    </div>
                    <div class="col-md-2">
                    <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/YihongGong.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/1gfa0LQsiQEq5awvXhTpJvq0V3657L7lD/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: This talk aims to argue for a fine(r)-grained perspective onto human-object interactions, from video sequences, captured in an 
                        egocentric perspective (i.e. first-person footage). Using multi-modal footage (appearance, motion, audio, language), I will present approaches for determining skill or expertise from video sequences [CVPR 2019], 
                        assessing action ‘completion’ – i.e. when an interaction is attempted but not completed [BMVC 2018], dual-domain and dual-time learning [CVPR 2020, CVPR 2019, ICCVW 2019] as well as multi-modal fusion using vision, 
                        audio and language [CVPR 2020, ICCV 2019, BMVC 2019]. All project details at: <a href="https://dimadamen.github.io/index.html#Projects">Link</a>. I will also introduce <a href="http://epic-kitchens.github.io">EPIC-KITCHENS-100</a>, 
                        the largest egocentric dataset in people’s homes. The dataset now includes 20M frames of 90K action segments and 100 hours of recording fully annotated, based on unique annotations from the participants narrating their own videos, 
                        thus reflecting true intention.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/JunweiHan.jpeg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="https://scholar.google.com/citations?user=xrqsoesAAAAJ&hl=en&oi=ao">Prof. Junwei Han</a></strong> is currently a Professor with the School of Automation, Northwestern Polytechnical University, China. His research interests include computer vision, pattern recognition, remote sensing image analysis, and brain imaging analysis. He has published more than 70 articles in top journals, such as the IEEE Transactions on Pattern Analysis and Machine Intelligence, the IEEE Transactions on Neural Networks and Learning Systems, and the International Journal of Computer Vision and more than 30 papers in top conferences, such as CVPR, ICCV, MICCAI, and IJCAI. He is an Associate Editor for several journals, such as the IEEE Transactions on Neural Networks and Learning Systems and the IEEE Transactions on Multimedia.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: Progress in video understanding has been astonishing in the past decade. Classifying, localizing, tracking and even segmenting actor instances at the pixel level is now common place, thanks to label-supervised machine learning. Yet, it is becoming increasingly clear that label-supervised knowledge transfer is expensive to obtain and scale, especially as the need for spatiotemporal detail and compositional semantic specification in long video sequences increases. In this talk we will discuss alternatives to label-supervision, using semantics, language, ontologies, similarity and time as the primary knowledge sources for various video understanding challenges. Despite being less example-dependent, the proposed algorithmic solutions are naturally embedded in modern (self-)-learned representations and lead to state-of-the-art unseen activity recognition in space and time.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-10">
                        <p class="large"><strong><a href="https://www.bbc.co.uk/rd/people/marta-mrak">Dr. Marta Mrak</a></strong> (Dipl. Ing.and MSc EE, University of Zagreb, Croatia; PhD, Queen Mary University of London, UK) is a Lead R&D Engineer at BBC R&D and an Honorary Professor at Queen Mary University of London (QMUL), Multimedia and Vision Research Group. Her research is focused on video compression fundamentals, new content experiences and data analytics. Before joining BBC R&D in 2010, Marta was a postdoctoral researcher at the University of Surrey and at QMUL. She is member of IEEE Signal Processing Society’s Technical Directions Board, Vice Chair of Technical Committee on Multimedia Signal Processing, and elected member of IEEE Circuits and Systems Society’s Technical Committee on Multimedia Systems and Applications. Marta has over 100 publications, including journal and conference papers, book chapters, patents, standardisation contributions, and two books. Her co-authored a paper on H.265/HEVC verification and evaluation methodology, received the IEEE CSVT Transactions Best Paper Award in 2017. Marta serves as General Co-Chair of IEEE ICME 2020 and Lead TPC Chair for IEEE ICME 2019. She has organised numerous workshops and special sessions at conferences, in addition to special issues in journals. During 2013-2018, Marta was the Area Editor for Signal Processing Image Communication journal. She was also appointed as Associate Editor for journal IEEE Transactions on Multimedia in 2018, and an Associate Editor for journal IEEE Transactions on Image Processing in 2019.</p>
                    </div>
                    <div class="col-md-2">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/MartaMrak.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Abstract</strong>: Sensing, understanding and synthesizing humans in images and videos have been a long-pursuing goal of computer vision and graphics, 
                        with extensive real-life applications. It is at the core of embodied intelligence. In this talk, I will discuss our work in human-centric visual analysis (of faces, human bodies, scenes, videos and 3D scans), 
                        with an emphasis on learning structural deep representations under complex scenarios. I will also discuss the challenges related to naturally-distributed data (e.g. long-tailed and open-ended) emerged from real-world sensors, 
                        and how we can overcome these challenges by incorporating new neural computing mechanisms such as dynamic memory and routing. Our approach has shown its effectiveness on both discriminative and generative tasks.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/chuanggan.jpg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="http://people.csail.mit.edu/ganchuang/">Dr. Chuang Gan</a></strong> is a research staff member at MIT-IBM Watson AI Lab. His research focus is on Computer Vision and Machine Learning. In particular, he is interested in multi-modal learning for video understanding. During his Ph.D., he has also spent time working at Stanford University, Carnegie Mellon University, University of Southern California, Google Research and Microsoft Research Redmond. Chuang is the recipient of Microsoft Graduate Fellowship and Baidu Graduate Fellowship in 2016. Chuang has won 1st Place in ActivityNet AVA Challenge 2018, 1st Place in ActivityNet Kinetics Challenge 2017, and 1st Place in NIST TRECVID MED and MER 2014. He is area chairs on ACM Multimedia 2019-2020, ICME 2019-2020, BMVC 2019, WACV 2019. He co-organized CVPR 2019 workshop on Multi-Modal learning from videos, and CVPR 2020 Tutorial on Neural-symbolic Visual Reasoning and Program Synthesis.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: Progress in video understanding has been astonishing in the past decade. Classifying, localizing, tracking and even segmenting actor instances at the pixel level is now common place, thanks to label-supervised machine learning. Yet, it is becoming increasingly clear that label-supervised knowledge transfer is expensive to obtain and scale, especially as the need for spatiotemporal detail and compositional semantic specification in long video sequences increases. In this talk we will discuss alternatives to label-supervision, using semantics, language, ontologies, similarity and time as the primary knowledge sources for various video understanding challenges. Despite being less example-dependent, the proposed algorithmic solutions are naturally embedded in modern (self-)-learned representations and lead to state-of-the-art unseen activity recognition in space and time.</p> -->
                    <hr />
                </div>
            </form>
            </div>
    </section><!-- #container -->
