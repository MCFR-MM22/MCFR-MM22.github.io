<!-- Portfolio Grid Section -->
    <section id="organizers" class="bg-white">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Keynote Speakers</h2>
<!--                     <h3 class="section-subheading text-muted">TBD</h3> -->
                </div>
            </div>
            <form id="searchForm" class="col-md-12 form-horizontal" role="form">
                <div class="row">
                    <div class="col-md-10">
                        <p class="large">
                        <strong><a href="http://gr.xjtu.edu.cn/web/ygong">Prof. Yihong Gong</a></strong> is a distinguished professor, an IEEE Fellow, the dean of School of Software Engineering of Xi’an Jiaotong University, a vice director of the National Engineering Laboratory for Visual Information Processing, and the acting vice director of the Shaanxi Province Joint Key AI Laboratories. His research interests include image/video content analysis, machine learning algorithms and human brain-inspired neural network models. He is among the first batch of researchers in the world initiating research studies on content-based image retrieval, sports video event detection, text/video content summarization, and image classification using the sparse coding image features. He has published more than 300 technical papers and two monographs. To date, his works have received more than 27,300 citations (Google h-index=69), with over 3,800 citations for his most cited paper.  In 2015, his ACM SIGIR 2003 paper titled “Document Clustering Based on Non-Negative Matrix Factorization” received “Test of Time Award” Honorable Mentions by the ACM SIGIR Technical Committee. Under his supervision, his teams have won numerous international/domestic competitions in image/video content recognitions. </p>
                    </div>
                    <div class="col-md-2">
                    <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/YihongGong.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: Brain-Inspired Machine Learning Methods</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/1gfa0LQsiQEq5awvXhTpJvq0V3657L7lD/view?usp=sharing">keynote-slides</a>]</p> -->
                    <p class="large"><strong>Abstract</strong>: Existing Deep Neural Networks (DNN) face the following three fundimental problems: (1) Rely on continuously increasing the network complexities and training data scales to improve the performance accuracies; (2) have the “texture-bias” problem when apllied for image classification; (3) have the “catastrophic forgetting” problem during the continual (or incremental) learning process.
In this talk, I will present several recent research results that are inspired by human brain visual cognitive mechanisms. First, we propose the CNN structure that is inspired by the dual-pathway object cognitive mechanism of the human visual system, which has proven to be effective for solving the “texture bias” problem of the existing DCNN models. To solve the “catastropic forgetting” problem that occurs during the continual learning process, we propose the Anchor Loss objective function that requires the DCNN model to keep the topological structure of the learned feature space. This work is inspired by the latest cognitive scientific research on human visual memory. Finally, I will present our few-shot continual learning mthod that can learn new image categories with few training samples, while keeping the image classification accuracies on the old image categories.
These proposed methods are independent of, and can be applied to any DCNN models. Comprehensive performance evaluations show remarkable performance improvements of the representative DCNN models on the respective tasks without increasing their model complexities. </p>
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/JunweiHan.jpeg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="https://scholar.google.com/citations?user=xrqsoesAAAAJ&hl=en&oi=ao">Prof. Junwei Han</a></strong> is currently a Professor with the School of Automation, Northwestern Polytechnical University, China. His research interests include computer vision, pattern recognition, remote sensing image analysis, and brain imaging analysis. He has published more than 70 articles in top journals, such as the IEEE Transactions on Pattern Analysis and Machine Intelligence, the IEEE Transactions on Neural Networks and Learning Systems, and the International Journal of Computer Vision and more than 30 papers in top conferences, such as CVPR, ICCV, MICCAI, and IJCAI. He is an Associate Editor for several journals, such as the IEEE Transactions on Neural Networks and Learning Systems and the IEEE Transactions on Multimedia.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: Modern Learning Methodologies for Co-Saliency Detection</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p> -->
                    <p class="large"><strong>Abstract</strong>:  Visual saliency computing aims to imitate the human visual attention mechanism to identify the most prominent or unique areas or objects from a visual scene. It is one of the basic low-level image processing techniques and can be applied to many downstream computer vision tasks. From the perspective of traditional research, visual saliency computing can be divided into eye fixation prediction and salient object detection. However, recent research progress shows that many new research directions and branches have emerged in this field, including weak/semi-unsupervised saliency learning, co-saliency detection, and multi-mode saliency detection. This report will focus on the key issues in co-saliency detection, introduce co-saliency methods based on advanced learning methods such as multi-instance learning, metric learning, and deep learning, and discuss potential future research directions in this research area.</p>
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-10">
                        <p class="large"><strong><a href="https://www.bbc.co.uk/rd/people/marta-mrak">Dr. Marta Mrak</a></strong> (Dipl. Ing.and MSc EE, University of Zagreb, Croatia; PhD, Queen Mary University of London, UK) is a Lead R&D Engineer at BBC R&D and an Honorary Professor at Queen Mary University of London (QMUL), Multimedia and Vision Research Group. Her research is focused on video compression fundamentals, new content experiences and data analytics. Before joining BBC R&D in 2010, Marta was a postdoctoral researcher at the University of Surrey and at QMUL. She is member of IEEE Signal Processing Society’s Technical Directions Board, Vice Chair of Technical Committee on Multimedia Signal Processing, and elected member of IEEE Circuits and Systems Society’s Technical Committee on Multimedia Systems and Applications. Marta has over 100 publications, including journal and conference papers, book chapters, patents, standardisation contributions, and two books. Her co-authored a paper on H.265/HEVC verification and evaluation methodology, received the IEEE CSVT Transactions Best Paper Award in 2017. Marta serves as General Co-Chair of IEEE ICME 2020 and Lead TPC Chair for IEEE ICME 2019. She has organised numerous workshops and special sessions at conferences, in addition to special issues in journals. During 2013-2018, Marta was the Area Editor for Signal Processing Image Communication journal. She was also appointed as Associate Editor for journal IEEE Transactions on Multimedia in 2018, and an Associate Editor for journal IEEE Transactions on Image Processing in 2019.</p>
                    </div>
                    <div class="col-md-2">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/MartaMrak.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Abstract</strong>: Sensing, understanding and synthesizing humans in images and videos have been a long-pursuing goal of computer vision and graphics, 
                        with extensive real-life applications. It is at the core of embodied intelligence. In this talk, I will discuss our work in human-centric visual analysis (of faces, human bodies, scenes, videos and 3D scans), 
                        with an emphasis on learning structural deep representations under complex scenarios. I will also discuss the challenges related to naturally-distributed data (e.g. long-tailed and open-ended) emerged from real-world sensors, 
                        and how we can overcome these challenges by incorporating new neural computing mechanisms such as dynamic memory and routing. Our approach has shown its effectiveness on both discriminative and generative tasks.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/chuanggan.jpg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="http://people.csail.mit.edu/ganchuang/">Dr. Chuang Gan</a></strong> is a research staff member at MIT-IBM Watson AI Lab. His research focus is on Computer Vision and Machine Learning. In particular, he is interested in multi-modal learning for video understanding. During his Ph.D., he has also spent time working at Stanford University, Carnegie Mellon University, University of Southern California, Google Research and Microsoft Research Redmond. Chuang is the recipient of Microsoft Graduate Fellowship and Baidu Graduate Fellowship in 2016. Chuang has won 1st Place in ActivityNet AVA Challenge 2018, 1st Place in ActivityNet Kinetics Challenge 2017, and 1st Place in NIST TRECVID MED and MER 2014. He is area chairs on ACM Multimedia 2019-2020, ICME 2019-2020, BMVC 2019, WACV 2019. He co-organized CVPR 2019 workshop on Multi-Modal learning from videos, and CVPR 2020 Tutorial on Neural-symbolic Visual Reasoning and Program Synthesis.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: Progress in video understanding has been astonishing in the past decade. Classifying, localizing, tracking and even segmenting actor instances at the pixel level is now common place, thanks to label-supervised machine learning. Yet, it is becoming increasingly clear that label-supervised knowledge transfer is expensive to obtain and scale, especially as the need for spatiotemporal detail and compositional semantic specification in long video sequences increases. In this talk we will discuss alternatives to label-supervision, using semantics, language, ontologies, similarity and time as the primary knowledge sources for various video understanding challenges. Despite being less example-dependent, the proposed algorithmic solutions are naturally embedded in modern (self-)-learned representations and lead to state-of-the-art unseen activity recognition in space and time.</p> -->
                    <hr />
                </div>
            </form>
            </div>
    </section><!-- #container -->
