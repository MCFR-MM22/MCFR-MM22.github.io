<!-- Portfolio Grid Section -->
    <section id="organizers" class="bg-white">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Keynote Speakers</h2>
<!--                     <h3 class="section-subheading text-muted">TBD</h3> -->
                </div>
            </div>
            <form id="searchForm" class="col-md-12 form-horizontal" role="form">
                <div class="row">
                    <div class="col-md-10">
                        <p class="large">
                        <strong><a href="http://gr.xjtu.edu.cn/web/ygong">Prof. Yihong Gong</a></strong> is a full Professor at the College of Software Engineering, Xi’an Jiaotong University, Xi’an, China. He received the B.S., M.S., and Ph.D. degrees in electrical and electronic engineering from The University of Tokyo, Tokyo, Japan, in 1987, 1989, and 1992, respectively. He was an Assistant Professor with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, for a period of four years. From 1996 to 1998, he was a Project Scientist with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA. He joined NEC Laboratories America, Princeton, NJ, USA, in 1999, and established the Media Analytics Group for the laboratories, where he became the Site Manager to lead the entire branch at Cupertino, CA, USA. He joined Xi’an Jiaotong University, Xi’an, China, in 2012, and became a Distinguished Professor with the National Thousand Talents Program, the Vice Director of the National Engineering Laboratory for Visual Information Processing, and the Chief Scientist of the China National Key Basic Research Project (973 Project). His current research interests include pattern recognition, machine learning, and multimedia content analysis. He is the Fellow of IEEE.</p>
                    </div>
                    <div class="col-md-2">
                    <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/YihongGong.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: Modern Learning Methodologies for Co-Saliency Detection</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/1gfa0LQsiQEq5awvXhTpJvq0V3657L7lD/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: Visual saliency computing aims to imitate the human visual attention mechanism to identify the most prominent or unique areas or objects from a visual scene. It is one of the basic low-level image processing techniques and can be applied to many downstream computer vision tasks. From the perspective of traditional research, visual saliency computing can be divided into eye fixation prediction and salient object detection. However, recent research progress shows that many new research directions and branches have emerged in this field, including weak/semi-unsupervised saliency learning, co-saliency detection, and multi-mode saliency detection. This report will focus on the key issues in co-saliency detection, introduce co-saliency methods based on advanced learning methods such as multi-instance learning, metric learning, and deep learning, and discuss potential future research directions in this research area.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/JunweiHan.jpeg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="https://scholar.google.com/citations?user=xrqsoesAAAAJ&hl=en&oi=ao">Prof. Junwei Han</a></strong> is currently a Professor with the School of Automation, Northwestern Polytechnical University, China. His research interests include computer vision, pattern recognition, remote sensing image analysis, and brain imaging analysis. He has published more than 70 articles in top journals, such as the IEEE Transactions on Pattern Analysis and Machine Intelligence, the IEEE Transactions on Neural Networks and Learning Systems, and the International Journal of Computer Vision and more than 30 papers in top conferences, such as CVPR, ICCV, MICCAI, and IJCAI. He is an Associate Editor for several journals, such as the IEEE Transactions on Neural Networks and Learning Systems and the IEEE Transactions on Multimedia.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: Modern Learning Methodologies for Co-Saliency Detection</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p> -->
                    <p class="large"><strong>Abstract</strong>:  Visual saliency computing aims to imitate the human visual attention mechanism to identify the most prominent or unique areas or objects from a visual scene. It is one of the basic low-level image processing techniques and can be applied to many downstream computer vision tasks. From the perspective of traditional research, visual saliency computing can be divided into eye fixation prediction and salient object detection. However, recent research progress shows that many new research directions and branches have emerged in this field, including weak/semi-unsupervised saliency learning, co-saliency detection, and multi-mode saliency detection. This report will focus on the key issues in co-saliency detection, introduce co-saliency methods based on advanced learning methods such as multi-instance learning, metric learning, and deep learning, and discuss potential future research directions in this research area.</p>
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-10">
                        <p class="large"><strong><a href="https://www.bbc.co.uk/rd/people/marta-mrak">Dr. Marta Mrak</a></strong> (Dipl. Ing.and MSc EE, University of Zagreb, Croatia; PhD, Queen Mary University of London, UK) is a Lead R&D Engineer at BBC R&D and an Honorary Professor at Queen Mary University of London (QMUL), Multimedia and Vision Research Group. Her research is focused on video compression fundamentals, new content experiences and data analytics. Before joining BBC R&D in 2010, Marta was a postdoctoral researcher at the University of Surrey and at QMUL. She is member of IEEE Signal Processing Society’s Technical Directions Board, Vice Chair of Technical Committee on Multimedia Signal Processing, and elected member of IEEE Circuits and Systems Society’s Technical Committee on Multimedia Systems and Applications. Marta has over 100 publications, including journal and conference papers, book chapters, patents, standardisation contributions, and two books. Her co-authored a paper on H.265/HEVC verification and evaluation methodology, received the IEEE CSVT Transactions Best Paper Award in 2017. Marta serves as General Co-Chair of IEEE ICME 2020 and Lead TPC Chair for IEEE ICME 2019. She has organised numerous workshops and special sessions at conferences, in addition to special issues in journals. During 2013-2018, Marta was the Area Editor for Signal Processing Image Communication journal. She was also appointed as Associate Editor for journal IEEE Transactions on Multimedia in 2018, and an Associate Editor for journal IEEE Transactions on Image Processing in 2019.</p>
                    </div>
                    <div class="col-md-2">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/MartaMrak.jpeg" alt="" width="224" height="224" />
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Abstract</strong>: Sensing, understanding and synthesizing humans in images and videos have been a long-pursuing goal of computer vision and graphics, 
                        with extensive real-life applications. It is at the core of embodied intelligence. In this talk, I will discuss our work in human-centric visual analysis (of faces, human bodies, scenes, videos and 3D scans), 
                        with an emphasis on learning structural deep representations under complex scenarios. I will also discuss the challenges related to naturally-distributed data (e.g. long-tailed and open-ended) emerged from real-world sensors, 
                        and how we can overcome these challenges by incorporating new neural computing mechanisms such as dynamic memory and routing. Our approach has shown its effectiveness on both discriminative and generative tasks.</p> -->
                    <hr />
                </div>
                <div class="row">
                    <div class="col-md-3">
                        <img class="alignnone size-medium wp-image-87 alignleft" src="../css/2021_style/img/speakers/chuanggan.jpg" alt="" width="224" height="224" />
                    </div>
                    <div class="col-md-9 ">
                        <p class="large"><strong><a href="http://people.csail.mit.edu/ganchuang/">Dr. Chuang Gan</a></strong> is a research staff member at MIT-IBM Watson AI Lab. His research focus is on Computer Vision and Machine Learning. In particular, he is interested in multi-modal learning for video understanding. During his Ph.D., he has also spent time working at Stanford University, Carnegie Mellon University, University of Southern California, Google Research and Microsoft Research Redmond. Chuang is the recipient of Microsoft Graduate Fellowship and Baidu Graduate Fellowship in 2016. Chuang has won 1st Place in ActivityNet AVA Challenge 2018, 1st Place in ActivityNet Kinetics Challenge 2017, and 1st Place in NIST TRECVID MED and MER 2014. He is area chairs on ACM Multimedia 2019-2020, ICME 2019-2020, BMVC 2019, WACV 2019. He co-organized CVPR 2019 workshop on Multi-Modal learning from videos, and CVPR 2020 Tutorial on Neural-symbolic Visual Reasoning and Program Synthesis.</p>
                    </div>
                </div>
                <div class="row">
                    <p class="large"><strong>Title</strong>: TBD</p>
<!--                     <p class="large"><strong>Keynote Slides</strong>: The download link is [<a href="https://drive.google.com/file/d/19G_P9SlqHXc2Bs4TC8WRztLGTeFmpI5h/view?usp=sharing">keynote-slides</a>]</p>
                    <p class="large"><strong>Abstract</strong>: Progress in video understanding has been astonishing in the past decade. Classifying, localizing, tracking and even segmenting actor instances at the pixel level is now common place, thanks to label-supervised machine learning. Yet, it is becoming increasingly clear that label-supervised knowledge transfer is expensive to obtain and scale, especially as the need for spatiotemporal detail and compositional semantic specification in long video sequences increases. In this talk we will discuss alternatives to label-supervision, using semantics, language, ontologies, similarity and time as the primary knowledge sources for various video understanding challenges. Despite being less example-dependent, the proposed algorithmic solutions are naturally embedded in modern (self-)-learned representations and lead to state-of-the-art unseen activity recognition in space and time.</p> -->
                    <hr />
                </div>
            </form>
            </div>
    </section><!-- #container -->
